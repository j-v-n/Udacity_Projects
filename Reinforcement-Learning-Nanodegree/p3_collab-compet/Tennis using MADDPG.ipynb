{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiagent Tennis - Competitive Solved Using Multi Agent - DDPG\n",
    "### Udacity Project Submission by Jayanth Nair\n",
    "This notebook walks through anthe implementation of the Multi-agent DDPG algorithm to solve the UnityML Tennis Environment (2 agents competing against each other)\n",
    "\n",
    "This solution utilizes 2 separate actor-critic networks for each of the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports to get environment up and running\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MADDPG Implementation Details\n",
    "\n",
    "#### Neural Network Architecture\n",
    "The DDPG network used in this notebook consists of two separate actor-critic networks.  The actor contains 3 fully connected layers and 1 output layer(24x128x64x2) with ReLU activation of the input and hidden layers.  The weights are initialized according to the DDPG paper\n",
    "\n",
    "The critic contains 3 fully connected layers and 1 output layer (24x128x64x1) with ReLU activation of input and hidden layers.\n",
    "\n",
    "The implementation used in this notebook is different to the original MADDPG paper. Instead of a critic which can see extra information during training (such as actions of other agents), the critics in this model only use the state observed by their respective agent\n",
    "\n",
    "Each actor-critic network has it's own experience replay buffer which contains observations of their respective agents.\n",
    "\n",
    "#### Hyper-parameters\n",
    "\n",
    "|Hyper-parameter |Value |\n",
    "|:-------------- |:---- |\n",
    "|Replay Buffer Size | 1000000 |\n",
    "|Batch Size | 512 |\n",
    "|Discount Factor | 0.99 |\n",
    "|Learning Rate-Actor | 0.0001 |\n",
    "|Learning Rate-Critic | 0.0001 |\n",
    "|L2 Weight Decay | 0 |\n",
    "|Update Network Every _ Steps | 1|\n",
    "|Learn For _ Times | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%aimport ddpg_multiagent \n",
    "from ddpg_multiagent_v2 import Agent\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_agents=states.shape[0] # number of agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing agent\n",
    "agent = Agent(state_size=state_size,action_size=action_size,random_seed=0,n_agents=n_agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddpg_multiagent(n_agents, n_episodes=10000, max_t=1000, print_every=100):\n",
    "    '''\n",
    "    Function which runs the training process \n",
    "\n",
    "    Arguments:\n",
    "     n_agents (int) : Number of agents\n",
    "     n_episodes (int): Max number of episodes to run training\n",
    "     max_t (int): Max time steps in each episode\n",
    "     print_every (int): Printing summarized results every x episodes\n",
    " \n",
    "    Returns :\n",
    "     scores (list): List of averaged scores over all agents for each episode \n",
    "\n",
    "\n",
    "    '''\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    best_score = 0.\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        agent.reset()\n",
    "        agent_scores = np.zeros(n_agents)\n",
    "        for t in range(max_t):\n",
    "            actions = [agent.act(states[agent_id], agent_id) for agent_id in range(n_agents)]\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones = env_info.local_done\n",
    "            for agent_id in range(n_agents):\n",
    "                agent_scores[agent_id] += rewards[agent_id]\n",
    "                agent.step(states[agent_id], actions[agent_id], rewards[agent_id], \n",
    "                           next_states[agent_id], dones[agent_id], agent_id)\n",
    "            states = next_states\n",
    "            if np.any(dones):\n",
    "                break\n",
    "        score = np.max(agent_scores)\n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode # - {0},\\tAverage Score - {1},\\tAverage Score over 100 Episodes-{2}'.format(i_episode, round(score,1), round(np.mean(scores_deque),1)),end=\" \")\n",
    "        if score > best_score:\n",
    "            print('\\rScore of {0} better than previous best score of {1} - Saving weights!'.format(round(score,1),round(best_score,1)))\n",
    "            for n in range(n_agents):\n",
    "                torch.save(agent.actor_local[n].state_dict(), 'checkpoint_actor_{}.pth'.format(n))\n",
    "                torch.save(agent.critic_local[n].state_dict(), 'checkpoint_critic_{}.pth'.format(n))\n",
    "            best_score = score\n",
    "        if i_episode % print_every == 0:\n",
    "            print('\\rEpisode # - {0},\\tAverage Score - {1},\\tAverage Score over 100 Episodes-{2}'.format(i_episode, round(score,1), round(np.mean(scores_deque),1)))\n",
    "        if score > 0.5 and np.mean(scores_deque) > 0.5:\n",
    "            print('\\rReacher environment with {0} agents solved in {1} episodes!'.format(n_agents,i_episode))\n",
    "            break\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode # - 8,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\udacity_reinforcement_learning\\Udacity Projects\\Reinforcement-Learning-Nanodegree\\p3_collab-compet\\ddpg_multiagent_v2.py:133: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  torch.nn.utils.clip_grad_norm(self.critic_local[agent_id].parameters(), 1) #clipping gradients as per tips from Udacity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of 0.1 better than previous best score of 0.0 - Saving weights!.0 \n",
      "Episode # - 100,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 200,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 300,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 400,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 500,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 600,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 700,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 800,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 900,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1000,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1100,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1200,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1300,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1400,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1500,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1600,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.0 \n",
      "Score of 0.2 better than previous best score of 0.1 - Saving weights!s-0.0 \n",
      "Episode # - 1700,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1800,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 1900,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 2000,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 2100,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 2200,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 2300,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 2400,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 2500,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.0 \n",
      "Episode # - 2600,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 2700,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 2800,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.1 \n",
      "Score of 0.3 better than previous best score of 0.2 - Saving weights!s-0.1 \n",
      "Episode # - 2900,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3000,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3100,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3200,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.1 \n",
      "Score of 0.6 better than previous best score of 0.3 - Saving weights!s-0.1 \n",
      "Episode # - 3300,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3400,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3500,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.1 \n",
      "Score of 0.7 better than previous best score of 0.6 - Saving weights!s-0.1 \n",
      "Episode # - 3600,\tAverage Score - 0.3,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3700,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3800,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.1 \n",
      "Episode # - 3900,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.1 \n",
      "Score of 1.1 better than previous best score of 0.7 - Saving weights!s-0.2 \n",
      "Episode # - 4000,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.2 \n",
      "Episode # - 4100,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.2 \n",
      "Episode # - 4200,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.2 \n",
      "Episode # - 4300,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.2 \n",
      "Episode # - 4400,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.2 \n",
      "Score of 1.2 better than previous best score of 1.1 - Saving weights!s-0.2 \n",
      "Episode # - 4500,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.2 \n",
      "Score of 1.5 better than previous best score of 1.2 - Saving weights!s-0.3 \n",
      "Episode # - 4600,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.2 \n",
      "Episode # - 4700,\tAverage Score - 0.4,\tAverage Score over 100 Episodes-0.3 \n",
      "Episode # - 4800,\tAverage Score - 0.2,\tAverage Score over 100 Episodes-0.2 \n",
      "Episode # - 4900,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.2 \n",
      "Score of 1.7 better than previous best score of 1.5 - Saving weights!s-0.2 \n",
      "Episode # - 5000,\tAverage Score - 1.7,\tAverage Score over 100 Episodes-0.2\n",
      "Episode # - 5100,\tAverage Score - 0.2,\tAverage Score over 100 Episodes-0.2 \n",
      "Episode # - 5200,\tAverage Score - 0.2,\tAverage Score over 100 Episodes-0.3 \n",
      "Episode # - 5300,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.3 \n",
      "Score of 2.1 better than previous best score of 1.7 - Saving weights!s-0.3 \n",
      "Episode # - 5400,\tAverage Score - 0.0,\tAverage Score over 100 Episodes-0.3 \n",
      "Episode # - 5500,\tAverage Score - 0.8,\tAverage Score over 100 Episodes-0.3 \n",
      "Score of 2.5 better than previous best score of 2.1 - Saving weights!s-0.3 \n",
      "Episode # - 5600,\tAverage Score - 0.6,\tAverage Score over 100 Episodes-0.4 \n",
      "Episode # - 5700,\tAverage Score - 0.1,\tAverage Score over 100 Episodes-0.4 \n",
      "Reacher environment with 2 agents solved in 5796 episodes!100 Episodes-0.5 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scores = ddpg_multiagent(n_agents)\n",
    "\n",
    "# plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
